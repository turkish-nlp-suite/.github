![nlp suite banner](/profile/nlp-suite-banner2.png)

:wave: Welcome to Turkish NLP Suite! 
======

[Turkish NLP Suite](https://www.turkish-nlp-suite.com) is a non-profit organization dedicated to Turkish NLP. We create open source corpora, pretrained models, code , tutorials and all types of linguistic resources for Turkish natural language processing. All of our code is cutting-edge, our models are easy to install and use, tutorials are great to get started .. This is state-of-art Turkish NLP after all.

:boom: We :heart: spaCy
=====
That's true, we love [spaCy](https://spacy.io/) because of the blazing fast code, great architecture, flexible pipelining, detailed documentation and awesome ecosystem. We proudly present spaCy Turkish models:

- tr_core_web_md
- tr_core_web_lg
- tr_core_web_trf  

All pipelines contains a tokenizer, trainable lemmatizer, POS tagger, dependency parser, morphologizer and NER components. You can find out more about each model in the [dedicated repo](https://github.com/turkish-nlp-suite/turkish-spacy-models) and download the models from [HuggingFace](https://huggingface.co/turkish-nlp-suite).

spaCy Turkish models comes with comprehensive tutorials and code. Please visit the documentation section for the details.
 
:sunglasses: We love cutting edge NLP
====
We corporate modern techniques into all our work including transformers, GPU computing as well as using the most efficient data structures. For some examples, the brand new Turkish spaCy model `tr_core_web_trf` is a transformer based pipeline; mini project "Quick FAQ Chatbot" integrates `sentence-transformers` and so on.  

:blue_book:  We love compiling datasets
====
Modern NLP revolves around data, hence labelled data (even in the msall amounts) are crucial for improving quality of many NLP tasks. As a result, compiling and serving Turkish datasets lies at the core of Turkish NLP Suite project. All of our datasets are presented with a commercial licence, completely open-source and ready to use. We also use our datasets in our projects and tutorials.
Here's a list of our datasets:

* [Corona-mini](https://github.com/turkish-nlp-suite/Corona-mini-dataset) : A mini corpus of Turkish social media reviews about Corona symptoms.
* [ATIS Turkish](https://github.com/turkish-nlp-suite/Atis_Turkish): A multi-purpose NLU dataset for Turkish, including entities and slots.
* [Turkish Wiki NER Dataset](https://github.com/turkish-nlp-suite/Turkish-Wiki-NER-Dataset): A general purpose Turkish NER Dataset with fine labels.
* [Vitamins and Supplements NER and Span Dataset](https://github.com/turkish-nlp-suite/Vitamins-Supplements-NER-dataset): A Turkish NER dataset that lies in intersection of medical NLP and product reviews.
* [Vitamins and Supplements Reviews](https://github.com/turkish-nlp-suite/Vitamins-Supplements-NER-dataset): A sentiment analysis dataset for Turkish, that lies in intersection of medical NLP and customer sentiment.
* [Beyazperde Top 300 Movies Dataset](https://github.com/turkish-nlp-suite/BeyazPerde-Movie-Reviews): Turkish sentiment analysis dataset of Top 300 Movies reviews.
* [Beyazperde Top All Movies Dataset](https://github.com/turkish-nlp-suite/BeyazPerde-Movie-Reviews): Turkish sentiment analysis dataset from movie reviews.

For the details and data please visit the dedicated repos of the datasets.
We also provide guidance and documentations for the ones who would like to compile their own datasets. If you're looking for creating your own datasets, please visit the documentation section.

:construction_worker: We love mining our datasets
====

:movie_camera: We love documentation: Turkish NLP Youtube Channel
====

Get started
====

Latest Medium Posts
====
